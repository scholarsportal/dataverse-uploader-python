# Feature: Automatic Retry of Failed Uploads

## ğŸ¯ Problem You Identified

You had to run the upload command **3 times** to upload all files:

```
Run 1: Uploaded 3, Failed 2  (employees, sales failed)
Run 2: Uploaded 1, Failed 1  (employees uploaded, sales failed)
Run 3: Uploaded 1, Failed 0  (sales finally uploaded)
```

**Why?** Dataverse was still processing previous uploads, causing temporary "Failed to add file to dataset" errors.

## âœ… Solution: Automatic Retry with Smart Delays

The uploader now automatically retries failed uploads with:
- **Waits 5 seconds** before first retry (lets Dataverse finish processing)
- **Retries up to 3 times** with increasing delays (5s, 10s, 20s)
- **Reloads file list** before each retry (detects files that finished processing)
- **2-second pause** between individual file retries

## ğŸ“¥ Download

[abstract_uploader_with_retry.py](computer:///mnt/user-data/outputs/abstract_uploader_with_retry.py)

## âš¡ Install

```powershell
cd C:\Users\mondesi6.utl\Downloads\dataverse-uploader-python
Copy-Item abstract_uploader_with_retry.py dataverse_uploader\core\abstract_uploader.py
poetry install
```

## ğŸ§ª How It Works

### Before (Manual Retries):
```powershell
# Run 1
dv-upload *.csv
# Files uploaded: 3, Failed: 2

# Wait... run again
dv-upload *.csv  
# Files uploaded: 1, Failed: 1

# Wait... run again
dv-upload *.csv
# Files uploaded: 1, Failed: 0
```

### After (Automatic Retries):
```powershell
# Run once
dv-upload *.csv

# Output:
# Files uploaded: 3
# Files failed: 2 (employees.csv, sales.csv)
# 
# ===== Retry attempt 1/3 for 2 failed file(s) =====
# Waiting 5 seconds for Dataverse to finish processing...
# Retrying: employees.csv
# Successfully uploaded on retry: employees.csv
# Retrying: sales.csv
# File now exists (was being processed): sales.csv
# 
# âœ“ All files uploaded successfully!
```

## ğŸ“Š Retry Strategy

| Attempt | Wait Time | What Happens |
|---------|-----------|--------------|
| Initial | 0s | Upload all files |
| Retry 1 | 5s | Retry failed files |
| Retry 2 | 10s | Retry still-failed files |
| Retry 3 | 20s | Final retry attempt |

Between each file retry: 2-second pause

## ğŸ¯ Smart Features

### 1. Detects Files Being Processed
```
Retrying: employees.csv
File now exists (was being processed): employees.csv
Skipped (already processed)
```

### 2. Reloads Dataset State
Before each retry, reloads the list of existing files to detect newly-processed uploads.

### 3. Exponential Backoff
- 1st retry: 5 seconds
- 2nd retry: 10 seconds  
- 3rd retry: 20 seconds

Gives Dataverse more time if it's slow.

### 4. Final Report
```
Files uploaded: 5
Files skipped: 1
Files failed: 0  â† All succeeded after retries!
```

## ğŸ§ª Test It

### Upload Files That Previously Failed:

```powershell
# This should now work in ONE run!
dv-upload *.csv
```

**Expected output:**
```
Processing file: customers.csv (564 bytes)
Successfully uploaded: customers.csv

Processing file: employees.csv (354 bytes)
Failed to upload employees.csv (Dataverse processing)

Processing file: products.csv (340 bytes)
Successfully uploaded: products.csv

Processing file: sales.csv (547 bytes)
Failed to upload sales.csv (Dataverse processing)

...

================================================================================
Retry attempt 1/3 for 2 failed file(s)
Waiting 5 seconds for Dataverse to finish processing...
================================================================================

Retrying: employees.csv
Successfully uploaded on retry: employees.csv

Retrying: sales.csv
Successfully uploaded on retry: sales.csv

âœ“ Upload complete!
Files uploaded: 6
Files skipped: 0
Files failed: 0
```

## âš™ï¸ Configuration

You can adjust retry behavior by modifying the code:

```python
# In abstract_uploader.py, _retry_failed_uploads method:

max_retry_attempts = 3  # Change to 5 for more retries
retry_delay = 5         # Change to 10 for longer initial wait
```

Or add to config (future enhancement):
```env
DV_MAX_RETRY_ATTEMPTS=5
DV_RETRY_DELAY=10
```

## ğŸ“ When Retries Help

Retries are useful for:
- âœ… **"Failed to add file to dataset"** - Dataset locked/processing
- âœ… **Temporary network issues** - Connection timeouts
- âœ… **Server busy** - 503/429 errors

Retries WON'T help for:
- âŒ **Invalid API key** - Authentication error (permanent)
- âŒ **File too large** - Size limit (permanent)
- âŒ **Corrupt file** - File issue (permanent)

## ğŸ‰ Benefits

1. **One Command**: Upload everything in one run
2. **No Manual Intervention**: No need to re-run manually
3. **Smart Waits**: Gives Dataverse time to process
4. **Better Success Rate**: Handles temporary failures
5. **Clear Logging**: Shows exactly what's being retried

## ğŸ” Log Example

```
2025-11-06 13:49:19 - Processing file: employees.csv
2025-11-06 13:49:20 - ERROR - Failed to upload employees.csv

2025-11-06 13:49:22 - ========================================
2025-11-06 13:49:22 - Retry attempt 1/3 for 2 failed file(s)
2025-11-06 13:49:22 - Waiting 5 seconds for Dataverse...
2025-11-06 13:49:22 - ========================================

2025-11-06 13:49:27 - Retrying: employees.csv
2025-11-06 13:49:28 - Successfully uploaded on retry: employees.csv

âœ“ Upload complete!
Files uploaded: 6 (including retries)
Files failed: 0
```

## ğŸ’¡ Pro Tips

### 1. Use With --verify
```powershell
dv-upload *.csv --verify
```
Combines duplicate detection with automatic retry!

### 2. Disable Retry (if needed)
```powershell
# Set max retries to 0 in config
# (Future: add --no-retry flag)
```

### 3. Watch the Logs
The logs show exactly what's happening during retries

### 4. For Large Batches
For 100+ files, retries are essential since Dataverse processing delays are common.

## ğŸš€ Quick Start

```powershell
# 1. Install the fix
Copy-Item abstract_uploader_with_retry.py dataverse_uploader\core\abstract_uploader.py
poetry install

# 2. Upload your files (just once!)
dv-upload *.csv

# That's it! The uploader handles retries automatically.
```

## ğŸ“Š Your Example - Before vs After

### Before (Manual):
```powershell
dv-upload *.csv          # Run 1: 3 uploaded, 2 failed
Start-Sleep -Seconds 10
dv-upload *.csv          # Run 2: 1 uploaded, 1 failed  
Start-Sleep -Seconds 10
dv-upload *.csv          # Run 3: 1 uploaded, 0 failed
```

### After (Automatic):
```powershell
dv-upload *.csv          # One run: 6 uploaded, 0 failed (after auto-retries)
```

---

Install the fix and never manually retry uploads again! ğŸŠ

## Summary

- âœ… Automatically retries failed uploads
- âœ… Smart delays (5s, 10s, 20s)
- âœ… Up to 3 retry attempts
- âœ… Detects files that finished processing
- âœ… Works with all existing features (--verify, --recurse, etc.)
- âœ… Clear logging of retry attempts

No more running `dv-upload` multiple times! ğŸ‰
